{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "723a672f7fb6dd08",
   "metadata": {},
   "source": [
    "# **Part 2: Feedforward network Case study – Multi-layer Perceptron model for predicting heat influx into a home**\n",
    "\n",
    "## 2. Develop feed-forward neural network models (Train few networks)\n",
    "\n",
    "### (ii) Gradient descent with adaptive learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T07:17:34.861945Z",
     "start_time": "2024-10-02T07:16:44.875077Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adagrad\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Configuration dictionary\n",
    "config = {\n",
    "    'batch_size': 10,\n",
    "    'epochs': 500,\n",
    "    'patience': 30,\n",
    "    'hidden_neurons': 5,  # Using the best number of neurons from previous model\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "def load_and_preprocess_data(seed):\n",
    "    \"\"\"Load and preprocess the data\"\"\"\n",
    "    # Load data\n",
    "    file_path = 'Heat_Influx_insulation_east_south_north.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Define features and target\n",
    "    features = ['Insulation', 'East', 'South', 'North']\n",
    "    target = 'HeatFlux'\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data_normalized = pd.DataFrame(\n",
    "        scaler.fit_transform(data[features + [target]]),\n",
    "        columns=features + [target]\n",
    "    )\n",
    "    \n",
    "    # Split the dataset (60% train, 20% validation, 20% test)\n",
    "    train_data, temp_data = train_test_split(data_normalized, train_size=0.6, random_state=seed)\n",
    "    val_data, test_data = train_test_split(temp_data, train_size=0.5, random_state=seed)\n",
    "    \n",
    "    # Prepare data sets\n",
    "    X_train = train_data[features].values\n",
    "    y_train = train_data[target].values\n",
    "    X_val = val_data[features].values\n",
    "    y_val = val_data[target].values\n",
    "    X_test = test_data[features].values\n",
    "    y_test = test_data[target].values\n",
    "    X_all = data_normalized[features].values\n",
    "    y_all = data_normalized[target].values\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_all, y_all\n",
    "\n",
    "def build_and_train_model_adagrad(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Build and train the model with Adagrad optimizer\"\"\"\n",
    "    K.clear_session()\n",
    "    tf.random.set_seed(config['seed'])\n",
    "    \n",
    "    # Define the model\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=config['seed'])\n",
    "    model = Sequential([\n",
    "        Input(shape=(4,)),\n",
    "        Dense(config['hidden_neurons'], activation='sigmoid', kernel_initializer=initializer),\n",
    "        Dense(1, activation='linear', kernel_initializer=initializer)\n",
    "    ])\n",
    "    \n",
    "    # Use Adagrad optimizer with default learning rate\n",
    "    optimizer = Adagrad()\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config['patience'],\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, X_all, y_all):\n",
    "    \"\"\"Evaluate the model and return metrics\"\"\"\n",
    "    y_train_pred = model.predict(X_train).flatten()\n",
    "    y_val_pred = model.predict(X_val).flatten()\n",
    "    y_test_pred = model.predict(X_test).flatten()\n",
    "    y_all_pred = model.predict(X_all).flatten()\n",
    "    \n",
    "    metrics = {\n",
    "        'MSE_Trn': mean_squared_error(y_train, y_train_pred),\n",
    "        'MSE_Val': mean_squared_error(y_val, y_val_pred),\n",
    "        'MSE_Test': mean_squared_error(y_test, y_test_pred),\n",
    "        'MSE_All': mean_squared_error(y_all, y_all_pred),\n",
    "        'R2_Trn': r2_score(y_train, y_train_pred),\n",
    "        'R2_Val': r2_score(y_val, y_val_pred),\n",
    "        'R2_Test': r2_score(y_test, y_test_pred),\n",
    "        'R2_All': r2_score(y_all, y_all_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_all_pred\n",
    "\n",
    "def plot_results(history, y_all, y_all_pred):\n",
    "    \"\"\"Plot training history and prediction results\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss During Training (Adagrad)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot predictions vs actual values\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(y_all, y_all_pred, alpha=0.5)\n",
    "    plt.plot([y_all.min(), y_all.max()], [y_all.min(), y_all.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Predictions vs Actual Values (Adagrad)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_results_adagrad.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, X_all, y_all = load_and_preprocess_data(config['seed'])\n",
    "    \n",
    "    print(\"\\nTraining model with Adagrad optimizer...\")\n",
    "    model_adagrad, history_adagrad = build_and_train_model_adagrad(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    print(\"\\nEvaluating model...\")\n",
    "    metrics_adagrad, y_all_pred_adagrad = evaluate_model(model_adagrad, X_train, y_train, X_val, y_val, X_test, y_test, X_all, y_all)\n",
    "    \n",
    "    print(\"\\nModel Performance Metrics (Adagrad):\")\n",
    "    for key, value in metrics_adagrad.items():\n",
    "        print(f\"{key}: {value:.6f}\")\n",
    "    \n",
    "    print(\"\\nPlotting results...\")\n",
    "    plot_results(history_adagrad, y_all, y_all_pred_adagrad)\n",
    "    \n",
    "    print(\"\\nSaving model...\")\n",
    "    save_model(model_adagrad, 'best_heat_flux_model_adagrad.keras')\n",
    "    \n",
    "    print(\"\\nProcess completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231c3e9b34c802a",
   "metadata": {},
   "source": [
    "### Results Table\n",
    "\n",
    "| Number of Neurons in Hidden Layer | MSE    |        |        |        | R²      |         |         |         |\n",
    "|----------------------------------|--------|--------|--------|--------|---------|---------|---------|---------|\n",
    "| 5                                | Trn    | Val    | Test   | All    | Trn     | Val     | Test    | All     |\n",
    "|                                  | 0.3291 | 0.2949 | 0.3095 | 0.3183 | -5.7059 | -4.9968 | -3.3841 | -4.9401 |\n",
    "\n",
    "### Analysis of Results\n",
    "\n",
    "The Adagrad optimizer, which automatically adapts the learning rate during training, showed significantly different performance compared to the constant learning rate method:\n",
    "\n",
    "1. The model demonstrated high Mean Squared Error (MSE) values across all datasets (training, validation, test, and all data).\n",
    "2. The negative R² values indicate that the model's predictions are worse than simply using the mean of the target values.\n",
    "3. The adaptive learning rate did not lead to better convergence in this case, suggesting potential issues with the optimization process.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
