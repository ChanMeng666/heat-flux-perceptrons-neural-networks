{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80c576e5b037ef14",
   "metadata": {},
   "source": [
    "# (c) Comparison and Selection of Best Network Model\n",
    "\n",
    "## Summary of Best Models for Each Hidden Layer Size\n",
    "\n",
    "| Hidden Neurons | Best Trial | Learning Rate | Momentum | Test MSE | Test R² | All Data MSE | All Data R² |\n",
    "|----------------|------------|---------------|----------|----------|---------|--------------|-------------|\n",
    "| 1 | A | 0.1 | 0.1 | 0.004940 | 0.897111 | 0.005050 | 0.905753 |\n",
    "| 3 | B | 0.1 | 0.9 | 0.003501 | 0.950399 | 0.003594 | 0.932930 |\n",
    "| 5 | B | 0.1 | 0.9 | 0.002905 | 0.958846 | 0.002996 | 0.944083 |\n",
    "\n",
    "## Best Model Selection\n",
    "\n",
    "Based on the results, the best overall model is:\n",
    "- Number of Hidden Neurons: 5\n",
    "- Trial B configuration:\n",
    "  * Learning Rate: 0.1\n",
    "  * Momentum: 0.9\n",
    "- Performance:\n",
    "  * Test MSE: 0.002905 (lowest among all models)\n",
    "  * Test R²: 0.958846 (highest among all models)\n",
    "  * All Data MSE: 0.002996\n",
    "  * All Data R²: 0.944083"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d66f4a4eb5b009",
   "metadata": {},
   "source": [
    "# (d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc69f0dd050654",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T06:56:28.506201Z",
     "start_time": "2024-10-02T06:55:35.513300Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# Configuration dictionary\n",
    "config = {\n",
    "    'batch_size': 10,\n",
    "    'epochs': 500,\n",
    "    'patience': 30,\n",
    "    'hidden_neurons': 5,\n",
    "    'learning_rate': 0.1,\n",
    "    'momentum': 0.9,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "def load_and_preprocess_data(seed):\n",
    "    \"\"\"Load and preprocess the data\"\"\"\n",
    "    # Load data\n",
    "    file_path = 'Heat_Influx_insulation_east_south_north.csv'\n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    # Define features and target\n",
    "    features = ['Insulation', 'East', 'South', 'North']\n",
    "    target = 'HeatFlux'\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data_normalized = pd.DataFrame(\n",
    "        scaler.fit_transform(data[features + [target]]),\n",
    "        columns=features + [target]\n",
    "    )\n",
    "    \n",
    "    # Split the dataset (60% train, 20% validation, 20% test)\n",
    "    train_data, temp_data = train_test_split(data_normalized, train_size=0.6, random_state=seed)\n",
    "    val_data, test_data = train_test_split(temp_data, train_size=0.5, random_state=seed)\n",
    "    \n",
    "    # Prepare data sets\n",
    "    X_train = train_data[features].values\n",
    "    y_train = train_data[target].values\n",
    "    X_val = val_data[features].values\n",
    "    y_val = val_data[target].values\n",
    "    X_test = test_data[features].values\n",
    "    y_test = test_data[target].values\n",
    "    X_all = data_normalized[features].values\n",
    "    y_all = data_normalized[target].values\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_all, y_all\n",
    "\n",
    "def build_and_train_model(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Build and train the model with the best configuration\"\"\"\n",
    "    K.clear_session()\n",
    "    tf.random.set_seed(config['seed'])\n",
    "    \n",
    "    # Define the model\n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=config['seed'])\n",
    "    model = Sequential([\n",
    "        Input(shape=(4,)),\n",
    "        Dense(config['hidden_neurons'], activation='sigmoid', kernel_initializer=initializer),\n",
    "        Dense(1, activation='linear', kernel_initializer=initializer)\n",
    "    ])\n",
    "    \n",
    "    optimizer = SGD(learning_rate=config['learning_rate'], momentum=config['momentum'])\n",
    "    model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "    \n",
    "    # Early stopping callback\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=config['patience'],\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=config['epochs'],\n",
    "        batch_size=config['batch_size'],\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, X_all, y_all):\n",
    "    \"\"\"Evaluate the model and return metrics\"\"\"\n",
    "    y_train_pred = model.predict(X_train).flatten()\n",
    "    y_val_pred = model.predict(X_val).flatten()\n",
    "    y_test_pred = model.predict(X_test).flatten()\n",
    "    y_all_pred = model.predict(X_all).flatten()\n",
    "    \n",
    "    metrics = {\n",
    "        'MSE_Trn': mean_squared_error(y_train, y_train_pred),\n",
    "        'MSE_Val': mean_squared_error(y_val, y_val_pred),\n",
    "        'MSE_Test': mean_squared_error(y_test, y_test_pred),\n",
    "        'MSE_All': mean_squared_error(y_all, y_all_pred),\n",
    "        'R2_Trn': r2_score(y_train, y_train_pred),\n",
    "        'R2_Val': r2_score(y_val, y_val_pred),\n",
    "        'R2_Test': r2_score(y_test, y_test_pred),\n",
    "        'R2_All': r2_score(y_all, y_all_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_all_pred\n",
    "\n",
    "def plot_results(history, y_all, y_all_pred):\n",
    "    \"\"\"Plot training history and prediction results\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.subplot(121)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss During Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot predictions vs actual values\n",
    "    plt.subplot(122)\n",
    "    plt.scatter(y_all, y_all_pred, alpha=0.5)\n",
    "    plt.plot([y_all.min(), y_all.max()], [y_all.min(), y_all.max()], 'r--', lw=2)\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title('Predictions vs Actual Values')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_results.png')\n",
    "    plt.close()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, X_all, y_all = load_and_preprocess_data(config['seed'])\n",
    "    \n",
    "    print(\"\\nTraining model with best configuration...\")\n",
    "    model, history = build_and_train_model(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    print(\"\\nEvaluating model...\")\n",
    "    metrics, y_all_pred = evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, X_all, y_all)\n",
    "    \n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value:.6f}\")\n",
    "    \n",
    "    print(\"\\nPlotting results...\")\n",
    "    plot_results(history, y_all, y_all_pred)\n",
    "    \n",
    "    print(\"\\nSaving model...\")\n",
    "    save_model(model, 'best_heat_flux_model.keras')\n",
    "    \n",
    "    print(\"\\nVerifying saved model...\")\n",
    "    loaded_model = load_model('best_heat_flux_model.keras')\n",
    "    loaded_metrics, _ = evaluate_model(loaded_model, X_train, y_train, X_val, y_val, X_test, y_test, X_all, y_all)\n",
    "    \n",
    "    print(\"\\nVerification - Loaded Model Performance:\")\n",
    "    for key, value in loaded_metrics.items():\n",
    "        print(f\"{key}: {value:.6f}\")\n",
    "    \n",
    "    print(\"\\nProcess completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947673b9a9953006",
   "metadata": {},
   "source": [
    "## **Key features of the saved model:**\n",
    "\n",
    "1. Architecture: 4 inputs → 5 hidden neurons (sigmoid activation) → 1 output (linear activation)\n",
    "2. Training configuration: \n",
    "   - Batch size: 10\n",
    "   - Maximum epochs: 500\n",
    "   - Early stopping with patience of 30\n",
    "   - Learning rate: 0.1\n",
    "   - Momentum: 0.9\n",
    "3. Initialization: Uses consistent seed (42) for reproducibility\n",
    "\n",
    "## **The code saves:**\n",
    "\n",
    "1. The complete model to a file named 'best_heat_flux_model'\n",
    "2. A plot of training history and predictions vs actual values as 'model_results.png'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
